{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wIQ8hjDpdVi"
      },
      "source": [
        "#### Importar lo necesario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHQUjDs12DLW"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJy8fjPn4wi"
      },
      "source": [
        "#### configuramos el `device` acorde al device disponible\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOV9xybtn4I3"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_tH9u082jpZ"
      },
      "source": [
        "\n",
        "#**MNIST data base**\n",
        "# Ejemplo de red neuronal de convolución (CNN)\n",
        "\n",
        "Vamos a usar la base de datos de MNIST ([ver fuente](http://yann.lecun.com/exdb/mnist/)) para entrenar una CNN que identifique números escritos a mano.\n",
        "\n",
        "Para esto necesitamos:\n",
        "\n",
        "\n",
        "1.   Cargar la base de datos.\n",
        "2.   Ver que la base de datos esté ok.\n",
        "3.   Construir nuestra CNN.\n",
        "4. Ver que las dimensiones de la red sean consistentes.\n",
        "4.   Definir funciones necesarias (de entrenamiento, de costo, etc.).\n",
        "5. Entrenar la red.\n",
        "6. Ver que funcione.\n",
        "7. Visualizar el kernel.\n",
        "8. Visualizar las activaciones.\n",
        "9.  Ejercicio nro 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQ-MLk6Do8e"
      },
      "source": [
        "## 1. Cargar base de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "X_test = torch.Tensor(np.resize(np.array(pickle.load( open( \"/content/test.pkl\", \"rb\" ) )), (7000, 1, 28, 28)))\n",
        "y_test = torch.Tensor(np.array(pickle.load( open( \"/content/test_label.pkl\", \"rb\" ) )))\n",
        "y_test = y_test.type(torch.LongTensor)\n",
        "X_train = torch.Tensor(np.resize(np.array(pickle.load( open( \"/content/train.pkl\", \"rb\" ) )), (56000, 1, 28, 28)))\n",
        "y_train = torch.Tensor(np.array(pickle.load( open( \"/content/train_label.pkl\", \"rb\" ))) )\n",
        "y_train = y_train.type(torch.LongTensor)\n",
        "X_val = torch.Tensor(np.resize(np.array(pickle.load( open( \"/content/val.pkl\", \"rb\" ) )), (7000, 1, 28, 28)))\n",
        "y_val = torch.Tensor(np.array(pickle.load( open( \"/content/val_label.pkl\", \"rb\" ))) )\n",
        "y_val = y_val.type(torch.LongTensor)\n",
        "print('X_train: ',X_train.shape, ' y_train: ',y_train.shape)\n",
        "print('X_test: ',X_test.shape, ' y_test: ',y_test.shape)\n",
        "print('X_val: ',X_val.shape, ' y_val: ',y_val.shape)\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "std = np.std(np.array(pickle.load( open( \"/content/test.pkl\", \"rb\" ) )))\n",
        "mean = np.mean(np.array(pickle.load( open( \"/content/test.pkl\", \"rb\" ) )))\n",
        "\n",
        "transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean,std)])\n",
        "X_train_t = transform(X_train)\n",
        "X_test_t = transform(X_test)\n",
        "X_val_t = transform(X_val)\n",
        "train_dataset = TensorDataset(X_train_t,y_train) \n",
        "test_dataset = TensorDataset(X_test_t,y_test) \n",
        "val_dataset = TensorDataset(X_val_t,y_val)\n",
        "\n",
        "dataloader = {\n",
        "    'train': DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True),\n",
        "    'test': DataLoader(test_dataset, batch_size=128, shuffle=False, pin_memory=True),\n",
        "    'val': DataLoader(val_dataset, batch_size=128, shuffle=False, pin_memory=True)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XflwPrTLEXpU",
        "outputId": "a6c97da4-7f68-4e1c-e40c-c5bb42911503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:  torch.Size([56000, 1, 28, 28])  y_train:  torch.Size([56000])\n",
            "X_test:  torch.Size([7000, 1, 28, 28])  y_test:  torch.Size([7000])\n",
            "X_val:  torch.Size([7000, 1, 28, 28])  y_val:  torch.Size([7000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikthAE4Dteb"
      },
      "source": [
        "## 2. Ver que la base de datos esté OK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyq2UFIl-Qjy",
        "outputId": "cb08b996-6501-4e6c-eef1-25896cdd77dd"
      },
      "source": [
        "print(type(dataloader))\n",
        "print(type(dataloader['train']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "p2fs6Qdivs1H",
        "outputId": "4b7ca875-6a0a-4823-a106-d8d8945ec2a8"
      },
      "source": [
        "# Ver imagen and label del dataloader (dataloader -> una herramienta para hacer batches de datasets)\n",
        "train_features, train_labels = next(iter(dataloader['train']))\n",
        "\n",
        "# verifico sus dimensiones\n",
        "print(f\"Tamaño del batch de feature (input / imagen): {train_features.size()}\")\n",
        "print(f\"Tamaño del batch del label (clase / etiqueta): {train_labels.size()}\")\n",
        "\n",
        "# tomo 1 imagen para poder visualizarla\n",
        "# y verifico sus dimensiones\n",
        "\n",
        "img = train_features[0]\n",
        "print('tamaño de 1 imagen: ', img.shape)\n",
        "# le QUITO 1 dimension (la del tamaño del batch) para poder graficar\n",
        "img = img.squeeze()\n",
        "print('tamaño de 1 imagen DESPUES de squeeze: ', img.shape)\n",
        "label = train_labels[0]\n",
        "\n",
        "# ploteo esa imagen\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del batch de feature (input / imagen): torch.Size([128, 1, 28, 28])\n",
            "Tamaño del batch del label (clase / etiqueta): torch.Size([128])\n",
            "tamaño de 1 imagen:  torch.Size([1, 28, 28])\n",
            "tamaño de 1 imagen DESPUES de squeeze:  torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0TN4erDxRd"
      },
      "source": [
        "## 3. Construyo mis CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrO5gfEL3KRC"
      },
      "source": [
        "# defino primero un \"bloque\" de una capa CNN\n",
        "# construido con los bloques funcionales vistos en clase\n",
        "#\n",
        "# (hiper)parámetros a pasar a la función:\n",
        "#   c_in:   canales (kernels) de entrada\n",
        "#   c_out:  canales (kernels) de salida\n",
        "#   k:      tamaño del kernel kxk\n",
        "#   p:      tamaño del padding de la convolución\n",
        "#   s:      stride de la convolución\n",
        "#   pk:     tamaño del kernel del pooling\n",
        "#   ps:     stride de la pooling\n",
        "#   pp:     padding en la pooling\n",
        "#\n",
        "#   la función pooling se elige directamente dentro del bloque!\n",
        "\n",
        "def block(c_in, c_out, k=3, p=1, s=1, pk=3, ps=2, pp=1):\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s), # conv\n",
        "        torch.nn.ReLU(),                                      # activation\n",
        "        torch.nn.MaxPool2d(pk, stride=ps, padding=pp)         # pooling\n",
        "    )\n",
        "\n",
        "\n",
        "# ahora SI construyo mi red... usando la clase CNN de pytorch\n",
        "# revisar muy bien las dimensiones a emplear en cada capa y\n",
        "# tener presente la reducción de las dimensiones.\n",
        "#\n",
        "# en la útlima capa fully conected 'fc', hacer bien el cálculo final del\n",
        "# tamaño del array que se obtiene siguiendo la formula vista en la teoria\n",
        "# tanto para la capa conv como para la capa pooling.\n",
        "#\n",
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self, n_channels=1, n_outputs=10):\n",
        "    super().__init__()\n",
        "    self.conv1 = block(n_channels, 64)\n",
        "    self.conv1_out = None #14 x 14\n",
        "    self.conv2 = block(64, 128) \n",
        "    self.conv2_out = None   # 7 x 7\n",
        "    self.conv3 = block(128, 254) \n",
        "    self.conv3_out = None  # 3 x 3\n",
        "    self.conv4 = block(254, 1568) \n",
        "    self.conv4_out = None  # 1 x 1\n",
        "    self.fc = torch.nn.Linear(1568*2*2, n_outputs) # verificar la dim de la salida para calcular el tamaño de la fully conected!!\n",
        "    self.sm = torch.nn.Softmax(dim=1)\n",
        "    print('Red creada')\n",
        "    print('arquitectura:')\n",
        "    print(self)\n",
        "    # Me fijo en el número de capas\n",
        "    self.i=0\n",
        "    for layer in self.children():\n",
        "        self.i = self.i+1\n",
        "    print('Número total de capas de CNN (conv+act+polling) + finales : ', self.i)\n",
        "    \n",
        "    # Me fijo en el número de parámetros entrenables\n",
        "    self.pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print('Número total de parámetros a entrenar: ', self.pytorch_total_params)\n",
        "\n",
        "  def validar_dim(self):\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    # es una funcion forward que imprime la dimension de cada paso\n",
        "    # la defino distinto de la forward standard para que cuando entrenemos\n",
        "    # no nos llene la pantalla de información inecesaria.\n",
        "\n",
        "    print(\"Validacion de dimensiones\")\n",
        "    tam = input(\"Ingrese tamaño de entrada: \")\n",
        "    x = torch.randn(1, 1, int(tam), int(tam))\n",
        "    print(\"Tamaño entrada: \", x.shape)\n",
        "    x = self.conv1(x)\n",
        "    print(\"Tamaño salida conv1: \", x.shape)\n",
        "    x = self.conv2(x)\n",
        "    print(\"Tamaño salida conv2: \", x.shape)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    print(\"Tamaño salida conv3: \", x.shape)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    print(\"Tamaño salida conv4: \", x.shape)\n",
        "\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    print(\"Tamaño imagen vectorizada: \", x.shape)\n",
        "    x = self.fc(x)\n",
        "    print(\"Tamaño salida fc (nro clases): \", x.shape)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.conv1_out = self.conv1(x)\n",
        "    self.conv2_out = self.conv2(self.conv1_out)\n",
        "    self.conv3_out = self.conv3(self.conv2_out)\n",
        "    self.conv4_out = self.conv4(self.conv3_out)\n",
        "    y = self.conv4_out.view(self.conv4_out.shape[0], -1)\n",
        "    y = self.fc(y)\n",
        "    x = self.sm(x)\n",
        "    return y\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6DoGaP31md",
        "outputId": "f5531eb2-6775-4673-d9e4-d7d2a1427299"
      },
      "source": [
        "model = CNN()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red creada\n",
            "arquitectura:\n",
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(128, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(254, 1568, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
            "  (sm): Softmax(dim=1)\n",
            ")\n",
            "Número total de capas de CNN (conv+act+polling) + finales :  6\n",
            "Número total de parámetros a entrenar:  4016104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4tEn-XqHVZ7"
      },
      "source": [
        "## 4. Veamos que las dimensiones sean consistentes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import ModuleList\n",
        "model.validar_dim()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKaOeug8DObE",
        "outputId": "18af0822-e51c-4468-a6b8-f505a4fe370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validacion de dimensiones\n",
            "Ingrese tamaño de entrada: 28\n",
            "Tamaño entrada:  torch.Size([1, 1, 28, 28])\n",
            "Tamaño salida conv1:  torch.Size([1, 64, 14, 14])\n",
            "Tamaño salida conv2:  torch.Size([1, 128, 7, 7])\n",
            "Tamaño salida conv3:  torch.Size([1, 254, 4, 4])\n",
            "Tamaño salida conv4:  torch.Size([1, 1568, 2, 2])\n",
            "Tamaño imagen vectorizada:  torch.Size([1, 6272])\n",
            "Tamaño salida fc (nro clases):  torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoB3GvDtGUgY"
      },
      "source": [
        "## 5. Armo las funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fM09lfI74jQ"
      },
      "source": [
        "from tqdm import tqdm # <- para graficar la barra de avance\n",
        "\n",
        "def fit(model, dataloader, epochs):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss, train_acc = [], []\n",
        "        bar = tqdm(dataloader['train'])\n",
        "        for batch in bar:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            ####\n",
        "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "            train_acc.append(acc)\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
        "        bar = tqdm(dataloader['test'])\n",
        "        val_loss, val_acc = [], []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "                val_acc.append(acc)\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "    acc_final = np.mean(val_acc)\n",
        "    \n",
        "    score = (10*acc_final*model.i)/(np.log(model.pytorch_total_params)*epochs)\n",
        "    print(\"score = \", score)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zagkkBhIG9Kc"
      },
      "source": [
        "## 6. Entreno la red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYHKN7XH7-Kk",
        "outputId": "bf4da630-59ef-4c97-85c0-c3577b8f53a5"
      },
      "source": [
        "epochs = 1\n",
        "fit(model, dataloader, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 0.31685 acc 0.91007: 100%|██████████| 438/438 [14:02<00:00,  1.92s/it]\n",
            "val_loss 0.07325 val_acc 0.97765: 100%|██████████| 55/55 [00:35<00:00,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 loss 0.31685 val_loss 0.07325 acc 0.91007 val_acc 0.97765\n",
            "score =  3.8576559306006866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra versión"
      ],
      "metadata": {
        "id": "kmy8V6nqbDmD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV88myhebGgE"
      },
      "source": [
        "# defino primero un \"bloque\" de una capa CNN\n",
        "# construido con los bloques funcionales vistos en clase\n",
        "#\n",
        "# (hiper)parámetros a pasar a la función:\n",
        "#   c_in:   canales (kernels) de entrada\n",
        "#   c_out:  canales (kernels) de salida\n",
        "#   k:      tamaño del kernel kxk\n",
        "#   p:      tamaño del padding de la convolución\n",
        "#   s:      stride de la convolución\n",
        "#   pk:     tamaño del kernel del pooling\n",
        "#   ps:     stride de la pooling\n",
        "#   pp:     padding en la pooling\n",
        "#\n",
        "#   la función pooling se elige directamente dentro del bloque!\n",
        "\n",
        "def block(c_in, c_out, k=3, p=1, s=1, pk=3, ps=2, pp=1):\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s), # conv\n",
        "        torch.nn.ReLU(),                                      # activation\n",
        "        torch.nn.MaxPool2d(pk, stride=ps, padding=pp)         # pooling\n",
        "    )\n",
        "\n",
        "\n",
        "# ahora SI construyo mi red... usando la clase CNN de pytorch\n",
        "# revisar muy bien las dimensiones a emplear en cada capa y\n",
        "# tener presente la reducción de las dimensiones.\n",
        "#\n",
        "# en la útlima capa fully conected 'fc', hacer bien el cálculo final del\n",
        "# tamaño del array que se obtiene siguiendo la formula vista en la teoria\n",
        "# tanto para la capa conv como para la capa pooling.\n",
        "#\n",
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self, n_channels=1, n_outputs=10):\n",
        "    super().__init__()\n",
        "    self.conv1 = block(n_channels, 64)\n",
        "    self.conv1_out = None #14 x 14\n",
        "    self.conv2 = block(64, 254) \n",
        "    self.conv2_out = None   # 7 x 7\n",
        "    self.conv3 = block(254, 392) \n",
        "    self.conv3_out = None  # 3 x 3\n",
        "    #self.conv4 = block(512, 1568) \n",
        "    #self.conv4_out = None  # 1 x 1\n",
        "    self.fc = torch.nn.Linear(392*4*4, n_outputs) # verificar la dim de la salida para calcular el tamaño de la fully conected!!\n",
        "    self.sm = torch.nn.Softmax(dim=1)\n",
        "    print('Red creada')\n",
        "    print('arquitectura:')\n",
        "    print(self)\n",
        "    # Me fijo en el número de capas\n",
        "    self.i=0\n",
        "    for layer in self.children():\n",
        "        self.i = self.i+1\n",
        "    print('Número total de capas de CNN (conv+act+polling) + finales : ', self.i)\n",
        "    \n",
        "    # Me fijo en el número de parámetros entrenables\n",
        "    self.pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print('Número total de parámetros a entrenar: ', self.pytorch_total_params)\n",
        "\n",
        "  def validar_dim(self):\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    # es una funcion forward que imprime la dimension de cada paso\n",
        "    # la defino distinto de la forward standard para que cuando entrenemos\n",
        "    # no nos llene la pantalla de información inecesaria.\n",
        "\n",
        "    print(\"Validacion de dimensiones\")\n",
        "    tam = input(\"Ingrese tamaño de entrada: \")\n",
        "    x = torch.randn(1, 1, int(tam), int(tam))\n",
        "    print(\"Tamaño entrada: \", x.shape)\n",
        "    x = self.conv1(x)\n",
        "    print(\"Tamaño salida conv1: \", x.shape)\n",
        "    x = self.conv2(x)\n",
        "    print(\"Tamaño salida conv2: \", x.shape)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    print(\"Tamaño salida conv3: \", x.shape)\n",
        "\n",
        "   # x = self.conv4(x)\n",
        "   # print(\"Tamaño salida conv4: \", x.shape)\n",
        "\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    print(\"Tamaño imagen vectorizada: \", x.shape)\n",
        "    x = self.fc(x)\n",
        "    print(\"Tamaño salida fc (nro clases): \", x.shape)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.conv1_out = self.conv1(x)\n",
        "    self.conv2_out = self.conv2(self.conv1_out)\n",
        "    self.conv3_out = self.conv3(self.conv2_out)\n",
        "   # self.conv4_out = self.conv4(self.conv3_out)\n",
        "    y = self.conv3_out.view(self.conv3_out.shape[0], -1)\n",
        "    y = self.fc(y)\n",
        "    x = self.sm(x)\n",
        "    return y\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMnC3Y6kbTQU",
        "outputId": "45e64a50-2ce1-4b42-dfee-754886d0841a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red creada\n",
            "arquitectura:\n",
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(64, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(254, 392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
            "  (sm): Softmax(dim=1)\n",
            ")\n",
            "Número total de capas de CNN (conv+act+polling) + finales :  5\n",
            "Número total de parámetros a entrenar:  1106432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import ModuleList\n",
        "model.validar_dim()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG6JBZgUbdhL",
        "outputId": "783476f5-f832-42ee-9bf6-348afc516ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validacion de dimensiones\n",
            "Ingrese tamaño de entrada: 28\n",
            "Tamaño entrada:  torch.Size([1, 1, 28, 28])\n",
            "Tamaño salida conv1:  torch.Size([1, 64, 14, 14])\n",
            "Tamaño salida conv2:  torch.Size([1, 254, 7, 7])\n",
            "Tamaño salida conv3:  torch.Size([1, 392, 4, 4])\n",
            "Tamaño imagen vectorizada:  torch.Size([1, 6272])\n",
            "Tamaño salida fc (nro clases):  torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWWPm7r2biRW"
      },
      "source": [
        "from tqdm import tqdm # <- para graficar la barra de avance\n",
        "\n",
        "def fit(model, dataloader, epochs):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss, train_acc = [], []\n",
        "        bar = tqdm(dataloader['train'])\n",
        "        for batch in bar:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            ####\n",
        "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "            train_acc.append(acc)\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
        "        bar = tqdm(dataloader['test'])\n",
        "        val_loss, val_acc = [], []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "                val_acc.append(acc)\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "    acc_final = np.mean(val_acc)\n",
        "    \n",
        "    score = (10*acc_final*model.i)/(np.log(model.pytorch_total_params)*epochs)\n",
        "    print(\"score = \", score)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "fit(model, dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ri_ULa1bkcf",
        "outputId": "8b05d5bf-9ced-4753-8919-9fe091be74bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 0.31395 acc 0.91658: 100%|██████████| 438/438 [10:00<00:00,  1.37s/it]\n",
            "val_loss 0.08535 val_acc 0.97253: 100%|██████████| 55/55 [00:29<00:00,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 loss 0.31395 val_loss 0.08535 acc 0.91658 val_acc 0.97253\n",
            "score =  3.4941365407157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra versión"
      ],
      "metadata": {
        "id": "xvJa6z1XLFsK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOOBY5MLHyC"
      },
      "source": [
        "# defino primero un \"bloque\" de una capa CNN\n",
        "# construido con los bloques funcionales vistos en clase\n",
        "#\n",
        "# (hiper)parámetros a pasar a la función:\n",
        "#   c_in:   canales (kernels) de entrada\n",
        "#   c_out:  canales (kernels) de salida\n",
        "#   k:      tamaño del kernel kxk\n",
        "#   p:      tamaño del padding de la convolución\n",
        "#   s:      stride de la convolución\n",
        "#   pk:     tamaño del kernel del pooling\n",
        "#   ps:     stride de la pooling\n",
        "#   pp:     padding en la pooling\n",
        "#\n",
        "#   la función pooling se elige directamente dentro del bloque!\n",
        "\n",
        "def block(c_in, c_out, k=3, p=1, s=1, pk=3, ps=4, pp=1):\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s), # conv\n",
        "        torch.nn.ReLU(),                                      # activation\n",
        "        torch.nn.MaxPool2d(pk, stride=ps, padding=pp)         # pooling\n",
        "    )\n",
        "\n",
        "\n",
        "# ahora SI construyo mi red... usando la clase CNN de pytorch\n",
        "# revisar muy bien las dimensiones a emplear en cada capa y\n",
        "# tener presente la reducción de las dimensiones.\n",
        "#\n",
        "# en la útlima capa fully conected 'fc', hacer bien el cálculo final del\n",
        "# tamaño del array que se obtiene siguiendo la formula vista en la teoria\n",
        "# tanto para la capa conv como para la capa pooling.\n",
        "#\n",
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self, n_channels=1, n_outputs=10):\n",
        "    super().__init__()\n",
        "    self.conv1 = block(n_channels, 64)\n",
        "    self.conv1_out = None #8 x 8\n",
        "    self.conv2 = block(64, 1568) \n",
        "    self.conv2_out = None   # 2 x 2\n",
        "    #self.conv3 = block(254, 392) \n",
        "    #self.conv3_out = None  # 3 x 3\n",
        "    #self.conv4 = block(512, 1568) \n",
        "    #self.conv4_out = None  # 1 x 1\n",
        "    self.fc = torch.nn.Linear(1568*2*2, n_outputs) # verificar la dim de la salida para calcular el tamaño de la fully conected!!\n",
        "    self.sm = torch.nn.Softmax(dim=1)\n",
        "    print('Red creada')\n",
        "    print('arquitectura:')\n",
        "    print(self)\n",
        "    # Me fijo en el número de capas\n",
        "    self.i=0\n",
        "    for layer in self.children():\n",
        "        self.i = self.i+1\n",
        "    print('Número total de capas de CNN (conv+act+polling) + finales : ', self.i)\n",
        "    \n",
        "    # Me fijo en el número de parámetros entrenables\n",
        "    self.pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print('Número total de parámetros a entrenar: ', self.pytorch_total_params)\n",
        "\n",
        "  def validar_dim(self):\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    # es una funcion forward que imprime la dimension de cada paso\n",
        "    # la defino distinto de la forward standard para que cuando entrenemos\n",
        "    # no nos llene la pantalla de información inecesaria.\n",
        "\n",
        "    print(\"Validacion de dimensiones\")\n",
        "    tam = input(\"Ingrese tamaño de entrada: \")\n",
        "    x = torch.randn(1, 1, int(tam), int(tam))\n",
        "    print(\"Tamaño entrada: \", x.shape)\n",
        "    x = self.conv1(x)\n",
        "    print(\"Tamaño salida conv1: \", x.shape)\n",
        "    x = self.conv2(x)\n",
        "    print(\"Tamaño salida conv2: \", x.shape)\n",
        "\n",
        "   # x = self.conv3(x)\n",
        "   # print(\"Tamaño salida conv3: \", x.shape)\n",
        "\n",
        "   # x = self.conv4(x)\n",
        "   # print(\"Tamaño salida conv4: \", x.shape)\n",
        "\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    print(\"Tamaño imagen vectorizada: \", x.shape)\n",
        "    x = self.fc(x)\n",
        "    print(\"Tamaño salida fc (nro clases): \", x.shape)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.conv1_out = self.conv1(x)\n",
        "    self.conv2_out = self.conv2(self.conv1_out)\n",
        "   # self.conv3_out = self.conv3(self.conv2_out)\n",
        "   # self.conv4_out = self.conv4(self.conv3_out)\n",
        "    y = self.conv2_out.view(self.conv2_out.shape[0], -1)\n",
        "    y = self.fc(y)\n",
        "    x = self.sm(x)\n",
        "    return y\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynR9n8tMVH9",
        "outputId": "9f090bc3-36fb-4d6e-ab64-31450fc25a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red creada\n",
            "arquitectura:\n",
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=4, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(64, 1568, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=3, stride=4, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
            "  (sm): Softmax(dim=1)\n",
            ")\n",
            "Número total de capas de CNN (conv+act+polling) + finales :  4\n",
            "Número total de parámetros a entrenar:  968106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import ModuleList\n",
        "model.validar_dim()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4xY7gKaMWDx",
        "outputId": "e79b4f1d-f645-4234-9386-a0733d850299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validacion de dimensiones\n",
            "Ingrese tamaño de entrada: 28\n",
            "Tamaño entrada:  torch.Size([1, 1, 28, 28])\n",
            "Tamaño salida conv1:  torch.Size([1, 64, 7, 7])\n",
            "Tamaño salida conv2:  torch.Size([1, 1568, 2, 2])\n",
            "Tamaño imagen vectorizada:  torch.Size([1, 6272])\n",
            "Tamaño salida fc (nro clases):  torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHiLWWwGMezF"
      },
      "source": [
        "from tqdm import tqdm # <- para graficar la barra de avance\n",
        "\n",
        "def fit(model, dataloader, epochs):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss, train_acc = [], []\n",
        "        bar = tqdm(dataloader['train'])\n",
        "        for batch in bar:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            ####\n",
        "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "            train_acc.append(acc)\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
        "        bar = tqdm(dataloader['test'])\n",
        "        val_loss, val_acc = [], []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "                val_acc.append(acc)\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "    acc_final = np.mean(val_acc)\n",
        "    \n",
        "    score = (10*acc_final*model.i)/(np.log(model.pytorch_total_params)*epochs)\n",
        "    print(\"score = \", score)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "fit(model, dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy2D6vGZMfmX",
        "outputId": "eb755c5b-05ce-4afc-8799-43407cf7005a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 0.48852 acc 0.88556: 100%|██████████| 438/438 [06:16<00:00,  1.16it/s]\n",
            "val_loss 0.15119 val_acc 0.96060: 100%|██████████| 55/55 [00:17<00:00,  3.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 loss 0.48852 val_loss 0.15119 acc 0.88556 val_acc 0.96060\n",
            "score =  2.787767555141407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}